# Ziggy Voice Assistant

A sophisticated, local-first, privacy-focused voice assistant with GPU acceleration, conversational abilities, and modular architecture.

## âœ¨ Key Features

- **ğŸ”’ Local-First Privacy**: No web search or online functionality - purely local processing
- **ğŸ¤ Wake Word Detection**: "Ziggy" activation using Vosk speech recognition  
- **ğŸ§  Dual AI Backend Support**: Works with both Msty and Ollama (auto-detects and can auto-start)
- **ğŸ’¬ Conversational Mode**: Natural multi-turn conversations without repeating wake word
- **âš¡ Resource Profiles**: Adaptive performance based on available GPU memory
- **ğŸ¯ Smart Query Routing**: Local functions for simple tasks, GPU-accelerated AI for complex queries
- **ğŸ—£ï¸ Voice Interruption**: Interrupt long responses by saying "Ziggy"
- **âš™ï¸ Modular Architecture**: Clean, testable, maintainable codebase
- **ğŸ§ª Comprehensive Testing**: 72% test coverage with automated unit tests

## ğŸ—ï¸ Architecture

This project features a clean, modular architecture that separates concerns and enables comprehensive testing:

### Original Monolithic Design
- Single 1665-line file with tightly coupled components
- Difficult to test and maintain
- Mixed responsibilities throughout

### New Modular Design
```
src/
â”œâ”€â”€ audio/          # Audio I/O abstraction layer
â”œâ”€â”€ speech/         # Speech recognition/synthesis with interruption
â”œâ”€â”€ ai/             # AI backend management (Ollama/Msty) 
â”œâ”€â”€ commands/       # Local command routing (time, date, math)
â”œâ”€â”€ conversation/   # Conversation context management
â”œâ”€â”€ resources/      # Resource profile management
â””â”€â”€ utils/          # Utility functions

tests/              # Comprehensive test suite
â”œâ”€â”€ test_audio.py   # Audio module tests
â”œâ”€â”€ test_speech.py  # Speech module tests
â”œâ”€â”€ test_ai_backend.py # AI backend tests
â”œâ”€â”€ test_commands.py   # Command routing tests
â”œâ”€â”€ test_conversation.py # Conversation tests
â””â”€â”€ test_resources.py  # Resource management tests
```

## ğŸ¯ Local-First Philosophy

Ziggy prioritizes privacy and local processing:
- **Local Functions**: Time, date, math, unit conversions handled without AI
- **No Online Access**: Web search functionality completely removed
- **Local AI Processing**: Uses your GPU for AI queries without sending data externally  
- **No Cloud Dependencies**: Everything runs on your hardware
- **Conversation History**: Stored in memory during session only

## ğŸ› ï¸ System Requirements

### Hardware
- **CPU**: Multi-core processor
- **GPU**: 
  - **Minimum**: 4GB VRAM (integrated graphics or older GPUs)
  - **Recommended**: 8GB+ VRAM (NVIDIA or AMD GPU)
  - **Optimal**: 16GB+ VRAM for extended conversations
- **RAM**: 4GB+ system RAM
- **Audio**: Microphone and speakers/headphones

### Software
- **OS**: Linux (tested on Ubuntu 24.04.1 LTS)
- **Python**: 3.10+
- **AI Backend**: Msty or Ollama (assistant can auto-start if needed)

## ğŸ“¦ Installation

### 1. Clone Repository
```bash
git clone https://github.com/JoshGK8/voice_assist
cd voice_assist
```

### 2. Install System Dependencies

<details>
<summary>Scripted Install (Recommended)</summary>

```bash
# Run the system prerequisites installer
chmod +x system_prerequisites.sh
./system_prerequisites.sh
```

</details>

<details>
<summary>Manual Install</summary>

Install the following packages:
```bash
# Audio and speech libraries
sudo apt install portaudio19-dev espeak espeak-data libespeak1 libespeak-dev

# Build tools
sudo apt install build-essential python3-dev curl

# Optional: Audio troubleshooting tools
sudo apt install alsa-utils pulseaudio-utils

# Add user to audio group
sudo usermod -a -G audio $USER
```

</details>

**Important**: Log out and back in after installation to apply group changes.

### 3. Set Up Python Environment
```bash
# Create virtual environment
python3 -m venv voice_assistant_env

# Activate environment
source voice_assistant_env/bin/activate

# Install Python dependencies
pip install -r requirements.txt

# Install test dependencies (optional)
pip install -r requirements-test.txt
```

### 4. Download Speech Recognition Model
```bash
# Download Vosk model (~50MB)
wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip
unzip vosk-model-small-en-us-0.15.zip
```

### 5. Set Up AI Backend (Optional)
The assistant will auto-detect running backends or offer to start one:

**Option A: Ollama** (Recommended)
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull a model (assistant will use first available)
ollama pull llama3.2
```

**Option B: Msty**
Download and install from [msty.app](https://msty.app)

### 6. Optional: Natural Voice Setup
For better voice quality than espeak:
```bash
chmod +x setup_piper.sh
./setup_piper.sh
```

## ğŸš€ Usage

### Available Versions

**Original Monolithic Version**:
```bash
source voice_assistant_env/bin/activate
python3 voice_assistant.py
```

**Clean Modular Version** (Recommended):
```bash
source voice_assistant_env/bin/activate
python3 voice_assistant_clean.py
```

Both versions have identical functionality, but the clean version uses the new modular architecture.

### Voice Commands

**Activation**: Say "Ziggy" to wake the assistant

**Conversational Mode**:
- Assistant automatically listens after asking questions
- No need to say "Ziggy" during conversations
- Maintains context across multiple exchanges
- Say "new conversation" to clear history

**Resource Profiles**:
- "Switch to gaming mode" - Minimal resource usage
- "Use standard profile" - Balanced performance  
- "Enable performance mode" - Maximum capabilities
- "What profile are you using?" - Check current status
- "How much memory?" - Get resource usage info

**Local Functions** (instant, no AI):
- "What time is it?"
- "What's the date?"
- "What's 25 plus 17?"
- "Convert 72 fahrenheit to celsius"
- "How many meters in 50 feet?"

**AI Queries** (uses local GPU):
- Complex questions and analysis
- Creative writing and coding help
- Technical explanations
- General knowledge (within training data)

**System Control**:
- "Take a break" - Shutdown assistant
- "Start over" / "New conversation" - Clear context
- "Ziggy" during long responses - Interrupt and start new command

### Conversational Flow Example
```
You: "Ziggy"
Ziggy: "Yes?"
You: "What's quantum computing?"
Ziggy: [Explains quantum computing and asks if you want to know more]
You: "Yes, how does it differ from regular computing?"  # No wake word needed!
Ziggy: [Continues explanation naturally]
```

## ğŸ§ª Testing

The modular architecture enables comprehensive testing:

```bash
# Run all tests
./run_tests.sh

# Or use make targets
make test                # Run all tests
make test-coverage      # Run with coverage report
make test-unit          # Run only unit tests

# Run specific test modules
pytest tests/test_audio.py -v
pytest tests/test_commands.py -v
```

**Test Coverage**: 72% overall with comprehensive unit tests for all modules.

## âš™ï¸ Resource Profiles

Ziggy adapts to your system capabilities:

### Minimal Profile (Gaming/Multitasking)
- **Requirements**: 4-8GB VRAM
- **Context**: 8,000 tokens
- **History**: 10 conversation turns
- **Recording**: 2 minutes conversational, 30 seconds commands
- **Use Case**: Gaming while using assistant, older GPUs, shared systems

### Standard Profile (Daily Use)
- **Requirements**: 8-16GB VRAM
- **Context**: 16,000 tokens  
- **History**: 20 conversation turns
- **Recording**: 5 minutes conversational, 1 minute commands
- **Use Case**: General productivity, balanced performance

### Performance Profile (Research/Extended Use)
- **Requirements**: 16GB+ VRAM
- **Context**: 32,000 tokens
- **History**: 50 conversation turns
- **Recording**: 10 minutes conversational, 2 minutes commands
- **Use Case**: Long research sessions, complex discussions

**Profile Switching**: The assistant auto-detects your GPU memory and selects an appropriate profile. You can switch profiles with voice commands.

## ğŸ”§ Configuration

### Customize Wake Word
Edit `voice_assistant_clean.py`:
```python
self.wake_word = "ziggy"  # Change to your preferred word
```

### Audio Settings
Adjust in the audio module configuration:
```python
sample_rate = 16000    # Audio sample rate
chunk_size = 1024      # Buffer size
```

## ğŸ› Troubleshooting

### Audio Issues
```bash
# List devices
aplay -l    # Playback devices
arecord -l  # Recording devices

# Test microphone
arecord -d 5 test.wav && aplay test.wav

# Check permissions
groups | grep audio
```

### GPU Detection
```bash
# NVIDIA
nvidia-smi

# AMD  
rocm-smi --showmeminfo vram
# or
cat /sys/class/drm/card*/device/mem_info_vram_total
```

### AI Backend Issues
```bash
# Check Ollama
curl http://localhost:11434/api/tags

# Check Msty
curl http://localhost:10002/v1/models

# The assistant will offer to start backends if not running
```

### Wake Word Problems
- Speak clearly and distinctly
- Reduce background noise
- Check microphone levels
- Consider environment acoustics

## ğŸ“Š Performance

### Resource Usage by Profile

| Profile | Idle RAM | Active RAM | GPU VRAM | Response Time |
|---------|----------|------------|----------|---------------|
| Minimal | ~100MB | ~200MB | 2-4GB | 1-3 seconds |
| Standard | ~150MB | ~300MB | 4-6GB | 1-5 seconds |
| Performance | ~200MB | ~400MB | 6-8GB | 2-8 seconds |

### Features by Profile

| Feature | Minimal | Standard | Performance |
|---------|---------|----------|-------------|
| Conversation Length | 10 turns | 20 turns | 50 turns |
| Context Window | 8K tokens | 16K tokens | 32K tokens |
| Recording Time | 2 min | 5 min | 10 min |
| Response Length | 500 tokens | 1000 tokens | 2000 tokens |

## ğŸ“ Project Structure
```
voice_assist/
â”œâ”€â”€ voice_assistant.py         # Original monolithic implementation
â”œâ”€â”€ voice_assistant_clean.py   # Clean modular implementation  
â”œâ”€â”€ src/                       # Modular source code
â”‚   â”œâ”€â”€ audio/                 # Audio I/O abstraction
â”‚   â”œâ”€â”€ speech/                # Speech processing & TTS
â”‚   â”œâ”€â”€ ai/                    # AI backend management
â”‚   â”œâ”€â”€ commands/              # Local command routing
â”‚   â”œâ”€â”€ conversation/          # Context management
â”‚   â””â”€â”€ resources/             # Resource profiles
â”œâ”€â”€ tests/                     # Comprehensive test suite
â”œâ”€â”€ pytest.ini                # Test configuration
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ requirements-test.txt      # Test dependencies
â”œâ”€â”€ Makefile                   # Build targets
â”œâ”€â”€ run_tests.sh              # Test runner script
â”œâ”€â”€ setup_piper.sh            # Natural voice setup
â””â”€â”€ system_prerequisites.sh   # System setup script
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch from main
3. Write tests for new functionality  
4. Ensure all tests pass: `make test`
5. Test all features work correctly
6. Ensure privacy principles are maintained
7. Submit pull request with clear description

## ğŸ“‹ Recent Updates

### Version 2.0 - Modular Architecture (Current)
- âœ… **Complete modular refactoring** - Clean, testable architecture
- âœ… **Comprehensive testing** - 72% test coverage with unit tests
- âœ… **Removed web search** - Purely local processing
- âœ… **Enhanced interruption handling** - Interrupt responses with wake word
- âœ… **Improved conversation management** - Better context handling
- âœ… **Resource profile management** - Dynamic GPU memory detection
- âœ… **Command routing system** - Smart local vs AI query routing

### Version 1.0 - Original Implementation
- âœ… Dual backend support (Msty/Ollama)
- âœ… Conversational mode with automatic listening
- âœ… Resource profiles for different hardware
- âœ… Voice-controlled profile switching
- âœ… Improved natural language command recognition  
- âœ… Piper TTS integration option
- âœ… Auto backend startup with voice selection

## ğŸ“ License

GPL-3.0 license

## ğŸ™ Acknowledgments

- **Vosk** for offline speech recognition
- **Ollama/Msty** for local AI backends
- **Piper TTS** for natural voice synthesis
- **espeak** for fallback text-to-speech
- **pytest** for comprehensive testing framework
- **Community** for testing and feedback

---

**Ziggy Voice Assistant** - Your local, private, AI-powered conversational companion with clean, maintainable code.